"""
Sanity Checks for 3D Synthetic Time Series Classification Datasets.

Compares 100 real datasets (from AEON UCR/UEA) with 100 synthetic datasets
generated by our 3D generator.

Checks:
1. Shape and difficulty (class balance, baseline accuracy, permutation test)
2. Predictive difficulty with simple models
3. Temporal signal localization
4. Temporal dependency analysis (ACF, cross-correlation)
5. Feature correlation analysis  
6. Calibration and separability
7. Real vs Synthetic discriminator
8. Leakage detection
"""

import numpy as np
import pickle
import json
import sys
import os
from pathlib import Path
from collections import Counter
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# Add paths
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent.parent / '01_real_data' / 'src'))

from generator import SyntheticDatasetGenerator3D, SyntheticDataset3D
from config import PriorConfig3D

# Try to import sklearn
try:
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Warning: sklearn not available. Some checks will be skipped.")

# Try to import xgboost
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False


@dataclass
class DatasetStats:
    """Statistics for a single dataset."""
    name: str
    n_samples: int
    n_features: int
    n_timesteps: int
    n_classes: int
    class_balance: float  # entropy of class distribution
    majority_class_ratio: float
    is_real: bool
    
    # Performance metrics (filled after model training)
    baseline_acc: float = 0.0
    logreg_acc: float = 0.0
    rf_acc: float = 0.0
    xgb_acc: float = 0.0
    
    # Time series specific models
    rocket_acc: float = 0.0  # ROCKET features + Ridge
    knn_dtw_acc: float = 0.0  # 1-NN with DTW (simplified)
    
    # Best model tracking
    best_model: str = ""
    best_acc: float = 0.0
    
    # Temporal stats
    acf_lag1_mean: float = 0.0
    acf_lag1_std: float = 0.0
    cross_corr_mean: float = 0.0
    
    # Signal localization
    perf_early: float = 0.0
    perf_mid: float = 0.0
    perf_late: float = 0.0
    
    # === Process characteristics (synthetic only) ===
    n_nodes: int = 0
    density: float = 0.0
    avg_skip: float = 0.0
    noise_scale: float = 0.0
    n_temporal_patterns: int = 0
    # Connection type counts
    n_self_connections: int = 0
    n_cross_connections: int = 0
    n_broadcast_connections: int = 0
    n_other_connections: int = 0


def load_real_datasets(pkl_path: Path, max_datasets: int = 100) -> List[Dict]:
    """Load real datasets from pickle file."""
    print(f"Loading real datasets from {pkl_path}...")
    
    # Add the src path for pickle to find the classes
    real_data_path = pkl_path.parent.parent.parent / 'src'
    if real_data_path.exists():
        sys.path.insert(0, str(real_data_path))
    
    # Also add the parent of src
    parent_path = pkl_path.parent.parent.parent
    if parent_path.exists():
        sys.path.insert(0, str(parent_path))
    
    with open(pkl_path, 'rb') as f:
        datasets = pickle.load(f)
    
    # Convert to standard format
    real_data = []
    for i, ds in enumerate(datasets[:max_datasets]):
        try:
            # Get X and y (combine train+test for analysis)
            if hasattr(ds, 'X_train') and ds.X_train is not None:
                X = np.concatenate([ds.X_train, ds.X_test], axis=0) if ds.X_test is not None else ds.X_train
                y = np.concatenate([ds.y_train, ds.y_test], axis=0) if ds.y_test is not None else ds.y_train
            else:
                X = ds.X
                y = ds.y
            
            if X is None or y is None:
                continue
                
            # Ensure X is 3D: (n_samples, n_features, n_timesteps)
            if X.ndim == 2:
                X = X[:, np.newaxis, :]  # (n, 1, t)
            
            # The data might be (n, t, m) - need to check and possibly transpose
            # AEON stores as (n, channels, length) or (n, length, channels)
            # We want (n, features, timesteps)
            # If last dim is small (< 20), it's probably channels
            if X.shape[2] < X.shape[1] and X.shape[2] < 20:
                # (n, timesteps, channels) -> (n, channels, timesteps)
                X = np.transpose(X, (0, 2, 1))
            
            # Encode labels to integers
            le = LabelEncoder()
            y = le.fit_transform(y)
            
            real_data.append({
                'name': ds.name if hasattr(ds, 'name') else f'real_{i}',
                'X': X.astype(np.float32),
                'y': y.astype(np.int32),
                'is_real': True
            })
        except Exception as e:
            print(f"  Error processing dataset {i}: {e}")
            continue
    
    print(f"  Loaded {len(real_data)} real datasets")
    return real_data


def generate_synthetic_datasets(n_datasets: int = 100, seed: int = 42) -> List[Dict]:
    """Generate synthetic datasets matching real data distribution."""
    print(f"Generating {n_datasets} synthetic datasets...")
    
    # Use default prior (already optimized in config.py for realistic series)
    # Key improvements:
    # - n_timesteps_range=(100, 1000) for longer series like real data
    # - prob_process_pure_ar=0.35 prioritizes simple AR-like processes
    # - prob_correlated_noise=0.85 with high AR coef for smooth series
    # - n_samples adjusted automatically based on n_classes * n_features
    prior = PriorConfig3D(
        n_samples_range=(100, 5000),  # More samples for complex problems
        n_features_range=(1, 15),
        # Use default timesteps from config (100-1000) - don't override!
        prob_univariate=0.5,  # Many real datasets are univariate
        max_classes=10,
    )
    
    generator = SyntheticDatasetGenerator3D(prior=prior, seed=seed)
    
    synth_data = []
    for i, dataset in enumerate(generator.generate_many(n_datasets)):
        # Extract process info from metadata
        process_info = dataset.metadata.get('process_info', {})
        conn_types = process_info.get('connection_types', {})
        
        synth_data.append({
            'name': f'synth_{i}',
            'X': dataset.X.astype(np.float32),
            'y': dataset.y.astype(np.int32),
            'is_real': False,
            # Process characteristics
            'process_info': {
                'n_nodes': process_info.get('n_nodes', 0),
                'density': process_info.get('density', 0.0),
                'avg_skip': process_info.get('avg_skip', 0.0),
                'noise_scale': process_info.get('noise_scale', 0.0),
                'n_temporal_patterns': dataset.metadata.get('n_temporal_patterns', 0),
                'n_self': conn_types.get('self', 0),
                'n_cross': conn_types.get('cross', 0),
                'n_broadcast': conn_types.get('broadcast_multiskip', 0),
                'n_other': sum(v for k, v in conn_types.items() 
                              if k not in ['self', 'cross', 'broadcast_multiskip']),
            }
        })
        if (i + 1) % 20 == 0:
            print(f"  Generated {i + 1}/{n_datasets}")
    
    print(f"  Generated {len(synth_data)} synthetic datasets")
    return synth_data


def extract_simple_features(X: np.ndarray) -> np.ndarray:
    """
    Extract simple features from time series for classification.
    
    X: (n_samples, n_features, n_timesteps)
    Returns: (n_samples, n_extracted_features)
    """
    n_samples, n_channels, n_timesteps = X.shape
    features = []
    
    for c in range(n_channels):
        channel_data = X[:, c, :]  # (n_samples, n_timesteps)
        
        # Basic stats
        features.append(np.mean(channel_data, axis=1))
        features.append(np.std(channel_data, axis=1))
        features.append(np.min(channel_data, axis=1))
        features.append(np.max(channel_data, axis=1))
        
        # Slope (linear regression)
        t = np.arange(n_timesteps)
        slopes = np.array([np.polyfit(t, channel_data[i], 1)[0] for i in range(n_samples)])
        features.append(slopes)
        
        # Autocorrelation lag 1
        if n_timesteps > 1:
            acf1 = np.array([np.corrcoef(channel_data[i, :-1], channel_data[i, 1:])[0, 1] 
                           if np.std(channel_data[i]) > 1e-10 else 0 
                           for i in range(n_samples)])
            features.append(np.nan_to_num(acf1))
        
        # Energy
        features.append(np.sum(channel_data**2, axis=1))
    
    return np.column_stack(features)


def compute_class_balance(y: np.ndarray) -> Tuple[float, float]:
    """Compute class balance metrics."""
    counts = Counter(y)
    total = len(y)
    probs = np.array([counts[c] / total for c in sorted(counts.keys())])
    
    # Entropy (normalized)
    entropy = -np.sum(probs * np.log(probs + 1e-10)) / np.log(len(counts) + 1e-10)
    
    # Majority class ratio
    majority_ratio = max(counts.values()) / total
    
    return entropy, majority_ratio


def compute_acf(X: np.ndarray, max_lag: int = 10) -> Tuple[float, float]:
    """Compute mean and std of ACF lag 1 across all channels."""
    n_samples, n_channels, n_timesteps = X.shape
    acf_values = []
    
    for c in range(n_channels):
        for i in range(min(n_samples, 100)):  # Sample for speed
            series = X[i, c, :]
            if np.std(series) > 1e-10 and len(series) > 1:
                acf = np.corrcoef(series[:-1], series[1:])[0, 1]
                if not np.isnan(acf):
                    acf_values.append(acf)
    
    if acf_values:
        return np.mean(acf_values), np.std(acf_values)
    return 0.0, 0.0


def compute_cross_correlation(X: np.ndarray) -> float:
    """Compute mean cross-channel correlation."""
    n_samples, n_channels, n_timesteps = X.shape
    
    if n_channels < 2:
        return 0.0
    
    correlations = []
    for i in range(min(n_samples, 50)):  # Sample for speed
        for c1 in range(n_channels):
            for c2 in range(c1 + 1, n_channels):
                if np.std(X[i, c1, :]) > 1e-10 and np.std(X[i, c2, :]) > 1e-10:
                    corr = np.corrcoef(X[i, c1, :], X[i, c2, :])[0, 1]
                    if not np.isnan(corr):
                        correlations.append(abs(corr))
    
    return np.mean(correlations) if correlations else 0.0


def train_and_evaluate(X: np.ndarray, y: np.ndarray, model_type: str = 'rf') -> float:
    """Train a model and return accuracy."""
    if not SKLEARN_AVAILABLE:
        return 0.0
    
    # Extract features
    X_feat = extract_simple_features(X)
    
    # Handle NaN/Inf
    X_feat = np.nan_to_num(X_feat, nan=0, posinf=0, neginf=0)
    
    # Train/test split
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X_feat, y, test_size=0.3, random_state=42, stratify=y
        )
    except ValueError:
        # If stratify fails (too few samples per class)
        X_train, X_test, y_train, y_test = train_test_split(
            X_feat, y, test_size=0.3, random_state=42
        )
    
    # Scale
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    # Train model
    if model_type == 'logreg':
        model = LogisticRegression(max_iter=500, random_state=42)
    elif model_type == 'rf':
        model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)
    elif model_type == 'xgb' and XGBOOST_AVAILABLE:
        model = XGBClassifier(n_estimators=50, max_depth=5, random_state=42, 
                             use_label_encoder=False, eval_metric='mlogloss')
    else:
        return 0.0
    
    try:
        model.fit(X_train, y_train)
        return accuracy_score(y_test, model.predict(X_test))
    except Exception:
        return 0.0


def extract_rocket_features(X: np.ndarray, n_kernels: int = 100) -> np.ndarray:
    """
    Simplified ROCKET-like features using random convolutional kernels.
    Not as powerful as real ROCKET but captures temporal patterns.
    
    X: (n_samples, n_features, n_timesteps)
    Returns: (n_samples, n_kernels * 2)  # PPV and max pooling per kernel
    """
    np.random.seed(42)
    n_samples, n_channels, n_timesteps = X.shape
    
    features = []
    
    for _ in range(n_kernels):
        # Random kernel length (7, 9, or 11)
        length = np.random.choice([7, 9, 11])
        length = min(length, n_timesteps - 1)
        
        # Random kernel weights
        kernel = np.random.randn(length)
        kernel = kernel - kernel.mean()  # Zero-mean
        
        # Random dilation
        dilation = np.random.randint(1, max(2, (n_timesteps - 1) // length))
        
        # Apply to each sample and channel
        ppv_all = []
        max_all = []
        
        for i in range(n_samples):
            ppv_sample = []
            max_sample = []
            for c in range(n_channels):
                series = X[i, c, :]
                
                # Simple convolution with dilation
                effective_len = length * dilation
                if effective_len >= n_timesteps:
                    ppv_sample.append(0)
                    max_sample.append(0)
                    continue
                
                conv_result = []
                for t in range(0, n_timesteps - effective_len):
                    indices = [t + j * dilation for j in range(length)]
                    val = np.sum(series[indices] * kernel)
                    conv_result.append(val)
                
                if conv_result:
                    ppv_sample.append(np.mean(np.array(conv_result) > 0))  # PPV
                    max_sample.append(np.max(conv_result))  # Max pooling
                else:
                    ppv_sample.append(0)
                    max_sample.append(0)
            
            ppv_all.append(np.mean(ppv_sample))
            max_all.append(np.mean(max_sample))
        
        features.append(ppv_all)
        features.append(max_all)
    
    return np.array(features).T


def train_rocket_classifier(X: np.ndarray, y: np.ndarray) -> float:
    """Train a classifier using ROCKET-like features."""
    if not SKLEARN_AVAILABLE:
        return 0.0
    
    try:
        # Extract ROCKET features
        X_rocket = extract_rocket_features(X, n_kernels=50)
        X_rocket = np.nan_to_num(X_rocket, nan=0, posinf=0, neginf=0)
        
        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X_rocket, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # Scale
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)
        
        # Ridge classifier (like real ROCKET)
        from sklearn.linear_model import RidgeClassifier
        model = RidgeClassifier(alpha=1.0)
        model.fit(X_train, y_train)
        
        return accuracy_score(y_test, model.predict(X_test))
    except Exception:
        return 0.0


def train_1nn_euclidean(X: np.ndarray, y: np.ndarray) -> float:
    """Train 1-NN with Euclidean distance on flattened series."""
    if not SKLEARN_AVAILABLE:
        return 0.0
    
    try:
        from sklearn.neighbors import KNeighborsClassifier
        
        # Flatten series
        X_flat = X.reshape(X.shape[0], -1)
        X_flat = np.nan_to_num(X_flat, nan=0, posinf=0, neginf=0)
        
        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X_flat, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # Scale
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)
        
        # 1-NN
        model = KNeighborsClassifier(n_neighbors=1)
        model.fit(X_train, y_train)
        
        return accuracy_score(y_test, model.predict(X_test))
    except Exception:
        return 0.0


def evaluate_temporal_slice(X: np.ndarray, y: np.ndarray, 
                           start_frac: float, end_frac: float) -> float:
    """Evaluate model on a temporal slice of the data."""
    n_timesteps = X.shape[2]
    start = int(start_frac * n_timesteps)
    end = int(end_frac * n_timesteps)
    
    X_slice = X[:, :, start:end]
    if X_slice.shape[2] < 3:
        return 0.0
    
    return train_and_evaluate(X_slice, y, 'rf')


def check_leakage(X: np.ndarray, y: np.ndarray) -> float:
    """Check for potential leakage by measuring max feature-target correlation."""
    X_feat = extract_simple_features(X)
    X_feat = np.nan_to_num(X_feat, nan=0, posinf=0, neginf=0)
    
    max_corr = 0.0
    for i in range(X_feat.shape[1]):
        try:
            corr = abs(np.corrcoef(X_feat[:, i], y)[0, 1])
            if not np.isnan(corr):
                max_corr = max(max_corr, corr)
        except:
            pass
    
    return max_corr


def permutation_test(X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:
    """
    Train with real labels vs permuted labels.
    Returns (real_acc, permuted_acc).
    """
    real_acc = train_and_evaluate(X, y, 'rf')
    
    y_perm = np.random.permutation(y)
    perm_acc = train_and_evaluate(X, y_perm, 'rf')
    
    return real_acc, perm_acc


def compute_dataset_stats(data: Dict, compute_models: bool = True) -> DatasetStats:
    """Compute all statistics for a dataset."""
    X, y = data['X'], data['y']
    n_samples, n_features, n_timesteps = X.shape
    n_classes = len(np.unique(y))
    
    # Class balance
    entropy, majority_ratio = compute_class_balance(y)
    
    stats = DatasetStats(
        name=data['name'],
        n_samples=n_samples,
        n_features=n_features,
        n_timesteps=n_timesteps,
        n_classes=n_classes,
        class_balance=entropy,
        majority_class_ratio=majority_ratio,
        is_real=data['is_real']
    )
    
    # Baseline accuracy (majority class)
    stats.baseline_acc = majority_ratio
    
    # Add process info for synthetic datasets
    if not data['is_real'] and 'process_info' in data:
        pinfo = data['process_info']
        stats.n_nodes = pinfo.get('n_nodes', 0)
        stats.density = pinfo.get('density', 0.0)
        stats.avg_skip = pinfo.get('avg_skip', 0.0)
        stats.noise_scale = pinfo.get('noise_scale', 0.0)
        stats.n_temporal_patterns = pinfo.get('n_temporal_patterns', 0)
        stats.n_self_connections = pinfo.get('n_self', 0)
        stats.n_cross_connections = pinfo.get('n_cross', 0)
        stats.n_broadcast_connections = pinfo.get('n_broadcast', 0)
        stats.n_other_connections = pinfo.get('n_other', 0)
    
    # Temporal stats
    stats.acf_lag1_mean, stats.acf_lag1_std = compute_acf(X)
    stats.cross_corr_mean = compute_cross_correlation(X)
    
    if compute_models and SKLEARN_AVAILABLE and n_samples >= 30:
        # Standard models on extracted features
        stats.logreg_acc = train_and_evaluate(X, y, 'logreg')
        stats.rf_acc = train_and_evaluate(X, y, 'rf')
        if XGBOOST_AVAILABLE:
            stats.xgb_acc = train_and_evaluate(X, y, 'xgb')
        
        # Time series specific models
        stats.rocket_acc = train_rocket_classifier(X, y)
        stats.knn_dtw_acc = train_1nn_euclidean(X, y)  # Using Euclidean as DTW proxy
        
        # Determine best model
        model_scores = {
            'logreg': stats.logreg_acc,
            'rf': stats.rf_acc,
            'xgb': stats.xgb_acc,
            'rocket': stats.rocket_acc,
            '1nn': stats.knn_dtw_acc
        }
        stats.best_model = max(model_scores, key=model_scores.get)
        stats.best_acc = model_scores[stats.best_model]
        
        # Signal localization
        stats.perf_early = evaluate_temporal_slice(X, y, 0.0, 0.33)
        stats.perf_mid = evaluate_temporal_slice(X, y, 0.33, 0.67)
        stats.perf_late = evaluate_temporal_slice(X, y, 0.67, 1.0)
    
    return stats


def train_discriminator(real_data: List[Dict], synth_data: List[Dict]) -> float:
    """
    Train a discriminator to distinguish real vs synthetic datasets.
    Returns AUC score.
    """
    if not SKLEARN_AVAILABLE:
        return 0.5
    
    print("\nTraining real vs synthetic discriminator...")
    
    features = []
    labels = []
    
    for data in real_data + synth_data:
        X, y = data['X'], data['y']
        
        # Dataset-level features
        n_samples, n_features, n_timesteps = X.shape
        entropy, majority_ratio = compute_class_balance(y)
        acf_mean, acf_std = compute_acf(X)
        cross_corr = compute_cross_correlation(X)
        
        # Global stats
        global_mean = np.mean(X)
        global_std = np.std(X)
        global_min = np.min(X)
        global_max = np.max(X)
        
        feat = [
            np.log(n_samples + 1),
            np.log(n_features + 1),
            np.log(n_timesteps + 1),
            len(np.unique(y)),
            entropy,
            majority_ratio,
            acf_mean,
            acf_std,
            cross_corr,
            global_mean,
            global_std,
            global_max - global_min,
        ]
        
        features.append(feat)
        labels.append(0 if data['is_real'] else 1)
    
    X = np.array(features)
    y = np.array(labels)
    
    # Handle NaN
    X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)
    
    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Train classifier
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)
    
    # Get AUC
    y_proba = clf.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
    
    # Feature importances
    importances = dict(zip(
        ['log_n_samples', 'log_n_features', 'log_n_timesteps', 'n_classes',
         'class_entropy', 'majority_ratio', 'acf_mean', 'acf_std', 
         'cross_corr', 'global_mean', 'global_std', 'range'],
        clf.feature_importances_
    ))
    
    print(f"  Discriminator AUC: {auc:.3f}")
    print(f"  Top discriminative features:")
    for feat, imp in sorted(importances.items(), key=lambda x: -x[1])[:5]:
        print(f"    {feat}: {imp:.3f}")
    
    return auc


def compare_distributions(real_stats: List[DatasetStats], 
                         synth_stats: List[DatasetStats]) -> Dict:
    """Compare distributions of metrics between real and synthetic."""
    
    def get_percentiles(values):
        return {
            'min': np.min(values),
            'p10': np.percentile(values, 10),
            'p25': np.percentile(values, 25),
            'median': np.median(values),
            'p75': np.percentile(values, 75),
            'p90': np.percentile(values, 90),
            'max': np.max(values),
            'mean': np.mean(values),
            'std': np.std(values)
        }
    
    comparison = {}
    
    metrics = [
        ('n_samples', 'Sample count'),
        ('n_features', 'Feature count'),
        ('n_timesteps', 'Timesteps'),
        ('n_classes', 'Number of classes'),
        ('class_balance', 'Class balance (entropy)'),
        ('majority_class_ratio', 'Majority class ratio'),
        ('baseline_acc', 'Baseline accuracy'),
        ('rf_acc', 'RandomForest accuracy'),
        ('rocket_acc', 'ROCKET accuracy'),
        ('knn_dtw_acc', '1-NN Euclidean accuracy'),
        ('best_acc', 'Best model accuracy'),
        ('acf_lag1_mean', 'ACF lag1 mean'),
        ('cross_corr_mean', 'Cross-channel correlation'),
    ]
    
    for metric, name in metrics:
        real_vals = [getattr(s, metric) for s in real_stats if getattr(s, metric) > 0]
        synth_vals = [getattr(s, metric) for s in synth_stats if getattr(s, metric) > 0]
        
        if real_vals and synth_vals:
            comparison[metric] = {
                'name': name,
                'real': get_percentiles(real_vals),
                'synthetic': get_percentiles(synth_vals)
            }
    
    return comparison


def run_sanity_checks(
    real_pkl_path: Optional[Path] = None,
    n_real: int = 100,
    n_synth: int = 100,
    compute_models: bool = True,
    output_path: Optional[Path] = None
) -> Dict:
    """
    Run all sanity checks comparing real vs synthetic datasets.
    """
    print("=" * 70)
    print("3D SYNTHETIC DATASET SANITY CHECKS")
    print("Real vs Synthetic Time Series Classification")
    print("=" * 70)
    
    # Load/generate datasets
    if real_pkl_path and real_pkl_path.exists():
        real_data = load_real_datasets(real_pkl_path, max_datasets=n_real)
    else:
        print("No real data pkl found. Using synthetic-only comparison.")
        real_data = []
    
    synth_data = generate_synthetic_datasets(n_datasets=n_synth)
    
    # Compute stats for all datasets
    print(f"\nComputing statistics for {len(real_data)} real + {len(synth_data)} synthetic datasets...")
    
    real_stats = []
    for i, data in enumerate(real_data):
        if (i + 1) % 20 == 0:
            print(f"  Real: {i + 1}/{len(real_data)}")
        try:
            stats = compute_dataset_stats(data, compute_models=compute_models)
            real_stats.append(stats)
        except Exception as e:
            print(f"  Error on real dataset {i}: {e}")
    
    synth_stats = []
    for i, data in enumerate(synth_data):
        if (i + 1) % 20 == 0:
            print(f"  Synthetic: {i + 1}/{len(synth_data)}")
        try:
            stats = compute_dataset_stats(data, compute_models=compute_models)
            synth_stats.append(stats)
        except Exception as e:
            print(f"  Error on synthetic dataset {i}: {e}")
    
    results = {
        'n_real': len(real_stats),
        'n_synthetic': len(synth_stats),
        'checks': {}
    }
    
    # ===== CHECK 1: Shape and Basic Stats =====
    print("\n" + "=" * 70)
    print("CHECK 1: SHAPE AND BASIC STATISTICS")
    print("=" * 70)
    
    comparison = compare_distributions(real_stats, synth_stats)
    results['checks']['distributions'] = comparison
    
    for metric, info in comparison.items():
        print(f"\n{info['name']}:")
        print(f"  Real:      min={info['real']['min']:.2f}, median={info['real']['median']:.2f}, max={info['real']['max']:.2f}, IQR=[{info['real']['p25']:.2f}, {info['real']['p75']:.2f}]")
        print(f"  Synthetic: min={info['synthetic']['min']:.2f}, median={info['synthetic']['median']:.2f}, max={info['synthetic']['max']:.2f}, IQR=[{info['synthetic']['p25']:.2f}, {info['synthetic']['p75']:.2f}]")
    
    # ===== CHECK 2: Baseline vs Model Performance =====
    print("\n" + "=" * 70)
    print("CHECK 2: BASELINE vs MODEL PERFORMANCE")
    print("=" * 70)
    
    if compute_models:
        # Calculate improvement over baseline
        real_improvements = [s.rf_acc - s.baseline_acc for s in real_stats if s.rf_acc > 0]
        synth_improvements = [s.rf_acc - s.baseline_acc for s in synth_stats if s.rf_acc > 0]
        
        if real_improvements:
            print(f"\nRF improvement over baseline:")
            print(f"  Real:      mean={np.mean(real_improvements):.3f}, median={np.median(real_improvements):.3f}")
        if synth_improvements:
            print(f"  Synthetic: mean={np.mean(synth_improvements):.3f}, median={np.median(synth_improvements):.3f}")
        
        results['checks']['model_improvements'] = {
            'real': {'mean': np.mean(real_improvements) if real_improvements else 0,
                    'median': np.median(real_improvements) if real_improvements else 0},
            'synthetic': {'mean': np.mean(synth_improvements) if synth_improvements else 0,
                         'median': np.median(synth_improvements) if synth_improvements else 0}
        }
    
    # ===== CHECK 3: Temporal Signal Localization =====
    print("\n" + "=" * 70)
    print("CHECK 3: TEMPORAL SIGNAL LOCALIZATION")
    print("=" * 70)
    
    if compute_models:
        for label, stats_list in [('Real', real_stats), ('Synthetic', synth_stats)]:
            early = [s.perf_early for s in stats_list if s.perf_early > 0]
            mid = [s.perf_mid for s in stats_list if s.perf_mid > 0]
            late = [s.perf_late for s in stats_list if s.perf_late > 0]
            
            if early and mid and late:
                print(f"\n{label} datasets - performance by temporal slice:")
                print(f"  Early (0-33%):  mean={np.mean(early):.3f}")
                print(f"  Mid (33-67%):   mean={np.mean(mid):.3f}")
                print(f"  Late (67-100%): mean={np.mean(late):.3f}")
    
    # ===== CHECK 4: Permutation Test =====
    print("\n" + "=" * 70)
    print("CHECK 4: PERMUTATION TEST (Label Shuffling)")
    print("=" * 70)
    
    # Run on a few datasets
    n_perm_tests = min(10, len(synth_data))
    perm_results = []
    for i, data in enumerate(synth_data[:n_perm_tests]):
        real_acc, perm_acc = permutation_test(data['X'], data['y'])
        perm_results.append({'real': real_acc, 'perm': perm_acc, 'drop': real_acc - perm_acc})
    
    if perm_results:
        avg_drop = np.mean([r['drop'] for r in perm_results])
        print(f"\nPermutation test on {n_perm_tests} synthetic datasets:")
        print(f"  Avg accuracy with real labels: {np.mean([r['real'] for r in perm_results]):.3f}")
        print(f"  Avg accuracy with permuted labels: {np.mean([r['perm'] for r in perm_results]):.3f}")
        print(f"  Avg drop: {avg_drop:.3f}")
        print(f"  {'PASS' if avg_drop > 0.1 else 'WARNING'}: Drop should be significant (>0.1)")
        
        results['checks']['permutation_test'] = {
            'avg_real_acc': np.mean([r['real'] for r in perm_results]),
            'avg_perm_acc': np.mean([r['perm'] for r in perm_results]),
            'avg_drop': avg_drop
        }
    
    # ===== CHECK 5: ACF and Temporal Dependencies =====
    print("\n" + "=" * 70)
    print("CHECK 5: TEMPORAL DEPENDENCIES (Autocorrelation)")
    print("=" * 70)
    
    real_acf = [s.acf_lag1_mean for s in real_stats if s.acf_lag1_mean != 0]
    synth_acf = [s.acf_lag1_mean for s in synth_stats if s.acf_lag1_mean != 0]
    
    if real_acf:
        print(f"\nACF lag-1 distribution:")
        print(f"  Real:      mean={np.mean(real_acf):.3f}, std={np.std(real_acf):.3f}")
    if synth_acf:
        print(f"  Synthetic: mean={np.mean(synth_acf):.3f}, std={np.std(synth_acf):.3f}")
    
    # ===== CHECK 6: Cross-Channel Correlation =====
    print("\n" + "=" * 70)
    print("CHECK 6: CROSS-CHANNEL CORRELATION")
    print("=" * 70)
    
    real_xcorr = [s.cross_corr_mean for s in real_stats if s.n_features > 1]
    synth_xcorr = [s.cross_corr_mean for s in synth_stats if s.n_features > 1]
    
    if real_xcorr:
        print(f"\nCross-channel correlation (multivariate only):")
        print(f"  Real:      mean={np.mean(real_xcorr):.3f}, std={np.std(real_xcorr):.3f}")
    if synth_xcorr:
        print(f"  Synthetic: mean={np.mean(synth_xcorr):.3f}, std={np.std(synth_xcorr):.3f}")
    
    # ===== CHECK 7: Leakage Detection =====
    print("\n" + "=" * 70)
    print("CHECK 7: LEAKAGE DETECTION")
    print("=" * 70)
    
    leakage_scores = []
    for data in synth_data[:20]:  # Sample for speed
        leakage = check_leakage(data['X'], data['y'])
        leakage_scores.append(leakage)
    
    if leakage_scores:
        max_leakage = max(leakage_scores)
        avg_leakage = np.mean(leakage_scores)
        print(f"\nMax feature-target correlation (synthetic):")
        print(f"  Average: {avg_leakage:.3f}")
        print(f"  Maximum: {max_leakage:.3f}")
        print(f"  {'PASS' if max_leakage < 0.9 else 'WARNING'}: Max should be < 0.9")
        
        results['checks']['leakage'] = {
            'avg': avg_leakage,
            'max': max_leakage
        }
    
    # ===== CHECK 8: Discriminator =====
    if real_data and synth_data:
        print("\n" + "=" * 70)
        print("CHECK 8: REAL vs SYNTHETIC DISCRIMINATOR")
        print("=" * 70)
        
        disc_auc = train_discriminator(real_data, synth_data)
        results['checks']['discriminator_auc'] = disc_auc
        
        print(f"\n  {'PASS' if disc_auc < 0.8 else 'WARNING'}: AUC should be < 0.8 (lower = more similar)")
    
    # ===== CHECK 9: Accuracy Distribution (Difficulty Spectrum) =====
    if compute_models:
        print("\n" + "=" * 70)
        print("CHECK 9: ACCURACY DISTRIBUTION (Difficulty Spectrum)")
        print("=" * 70)
        
        for label, stats_list in [('Real', real_stats), ('Synthetic', synth_stats)]:
            best_accs = [s.best_acc for s in stats_list if s.best_acc > 0]
            if best_accs:
                print(f"\n{label} - Best model accuracy distribution:")
                print(f"  Min:    {np.min(best_accs):.3f}")
                print(f"  P10:    {np.percentile(best_accs, 10):.3f}")
                print(f"  P25:    {np.percentile(best_accs, 25):.3f}")
                print(f"  Median: {np.median(best_accs):.3f}")
                print(f"  P75:    {np.percentile(best_accs, 75):.3f}")
                print(f"  P90:    {np.percentile(best_accs, 90):.3f}")
                print(f"  Max:    {np.max(best_accs):.3f}")
                
                # Categorize difficulty
                easy = sum(1 for a in best_accs if a > 0.9)
                medium = sum(1 for a in best_accs if 0.5 <= a <= 0.9)
                hard = sum(1 for a in best_accs if a < 0.5)
                print(f"  Easy (>0.9): {easy}/{len(best_accs)} ({100*easy/len(best_accs):.1f}%)")
                print(f"  Medium (0.5-0.9): {medium}/{len(best_accs)} ({100*medium/len(best_accs):.1f}%)")
                print(f"  Hard (<0.5): {hard}/{len(best_accs)} ({100*hard/len(best_accs):.1f}%)")
    
    # ===== CHECK 10: Model Ranking (Which model wins?) =====
    if compute_models:
        print("\n" + "=" * 70)
        print("CHECK 10: MODEL RANKING (Which model wins on each dataset?)")
        print("=" * 70)
        
        for label, stats_list in [('Real', real_stats), ('Synthetic', synth_stats)]:
            model_wins = Counter([s.best_model for s in stats_list if s.best_model])
            if model_wins:
                print(f"\n{label} - Best model counts:")
                total = sum(model_wins.values())
                for model, count in model_wins.most_common():
                    print(f"  {model}: {count}/{total} ({100*count/total:.1f}%)")
        
        # Check if there's diversity (no single model dominates)
        synth_wins = Counter([s.best_model for s in synth_stats if s.best_model])
        if synth_wins:
            top_model_pct = synth_wins.most_common(1)[0][1] / sum(synth_wins.values())
            print(f"\n  {'PASS' if top_model_pct < 0.6 else 'WARNING'}: Top model should win < 60% (diversity check)")
    
    # ===== CHECK 11: All Models Performance Comparison =====
    if compute_models:
        print("\n" + "=" * 70)
        print("CHECK 11: ALL MODELS PERFORMANCE COMPARISON")
        print("=" * 70)
        
        models = ['logreg', 'rf', 'xgb', 'rocket', '1nn']
        model_attrs = ['logreg_acc', 'rf_acc', 'xgb_acc', 'rocket_acc', 'knn_dtw_acc']
        
        for label, stats_list in [('Real', real_stats), ('Synthetic', synth_stats)]:
            print(f"\n{label} datasets - model performance (median accuracy):")
            for model, attr in zip(models, model_attrs):
                accs = [getattr(s, attr) for s in stats_list if getattr(s, attr) > 0]
                if accs:
                    print(f"  {model:8s}: min={np.min(accs):.3f}, median={np.median(accs):.3f}, max={np.max(accs):.3f}")
        
        results['checks']['model_comparison'] = {
            'real': {model: np.median([getattr(s, attr) for s in real_stats if getattr(s, attr) > 0]) 
                    for model, attr in zip(models, model_attrs)},
            'synthetic': {model: np.median([getattr(s, attr) for s in synth_stats if getattr(s, attr) > 0]) 
                         for model, attr in zip(models, model_attrs)}
        }
    
    # ===== SUMMARY =====
    print("\n" + "=" * 70)
    print("SANITY CHECK SUMMARY")
    print("=" * 70)
    
    print(f"\nDatasets analyzed: {len(real_stats)} real, {len(synth_stats)} synthetic")
    
    # ===== PROCESS CHARACTERISTICS SUMMARY (Synthetic only) =====
    print("\n" + "=" * 70)
    print("SYNTHETIC PROCESS CHARACTERISTICS")
    print("=" * 70)
    
    # Compute summary of process characteristics
    if synth_stats:
        n_nodes_list = [s.n_nodes for s in synth_stats if s.n_nodes > 0]
        density_list = [s.density for s in synth_stats if s.density > 0]
        avg_skip_list = [s.avg_skip for s in synth_stats if s.avg_skip > 0]
        noise_scale_list = [s.noise_scale for s in synth_stats if s.noise_scale > 0]
        
        n_self = sum(s.n_self_connections for s in synth_stats)
        n_cross = sum(s.n_cross_connections for s in synth_stats)
        n_broadcast = sum(s.n_broadcast_connections for s in synth_stats)
        n_other = sum(s.n_other_connections for s in synth_stats)
        n_total_conn = n_self + n_cross + n_broadcast + n_other
        
        if n_nodes_list:
            print(f"\nGraph structure:")
            print(f"  n_nodes:     min={np.min(n_nodes_list):.0f}, median={np.median(n_nodes_list):.0f}, max={np.max(n_nodes_list):.0f}")
        if density_list:
            print(f"  density:     min={np.min(density_list):.2f}, median={np.median(density_list):.2f}, max={np.max(density_list):.2f}")
        if avg_skip_list:
            print(f"  avg_skip:    min={np.min(avg_skip_list):.1f}, median={np.median(avg_skip_list):.1f}, max={np.max(avg_skip_list):.1f}")
        if noise_scale_list:
            print(f"  noise_scale: min={np.min(noise_scale_list):.3f}, median={np.median(noise_scale_list):.3f}, max={np.max(noise_scale_list):.3f}")
        
        if n_total_conn > 0:
            print(f"\nConnection types distribution:")
            print(f"  self:      {n_self}/{n_total_conn} ({100*n_self/n_total_conn:.1f}%)")
            print(f"  cross:     {n_cross}/{n_total_conn} ({100*n_cross/n_total_conn:.1f}%)")
            print(f"  broadcast: {n_broadcast}/{n_total_conn} ({100*n_broadcast/n_total_conn:.1f}%)")
            print(f"  other:     {n_other}/{n_total_conn} ({100*n_other/n_total_conn:.1f}%)")
        
        # Save process summary
        results['checks']['process_characteristics'] = {
            'n_nodes': {'min': np.min(n_nodes_list), 'median': np.median(n_nodes_list), 'max': np.max(n_nodes_list)} if n_nodes_list else {},
            'density': {'min': np.min(density_list), 'median': np.median(density_list), 'max': np.max(density_list)} if density_list else {},
            'avg_skip': {'min': np.min(avg_skip_list), 'median': np.median(avg_skip_list), 'max': np.max(avg_skip_list)} if avg_skip_list else {},
            'noise_scale': {'min': np.min(noise_scale_list), 'median': np.median(noise_scale_list), 'max': np.max(noise_scale_list)} if noise_scale_list else {},
            'connection_types': {
                'self': n_self,
                'cross': n_cross,
                'broadcast': n_broadcast,
                'other': n_other,
                'total': n_total_conn
            }
        }
    
    # Save results
    if output_path:
        # Convert stats to serializable format
        results['real_stats'] = [vars(s) for s in real_stats]
        results['synth_stats'] = [vars(s) for s in synth_stats]
        
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nResults saved to {output_path}")
    
    return results


def main():
    """Main entry point."""
    # Find real data
    real_pkl = Path(__file__).parent.parent / '01_real_data' / 'AEON' / 'data' / 'classification_datasets.pkl'
    
    # Output path
    output_path = Path(__file__).parent / 'sanity_check_results.json'
    
    # Run checks
    results = run_sanity_checks(
        real_pkl_path=real_pkl,
        n_real=100,
        n_synth=100,
        compute_models=True,
        output_path=output_path
    )
    
    print("\n" + "=" * 70)
    print("DONE!")
    print("=" * 70)


if __name__ == "__main__":
    main()

