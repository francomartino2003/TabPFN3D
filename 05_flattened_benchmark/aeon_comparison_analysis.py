#!/usr/bin/env python3
"""
Compare AEON benchmarks with TabPFN (baseline and finetuned) on flattenable datasets.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

# Paths
AEON_BENCHMARKS_DIR = Path("/Users/franco/Documents/TabPFN3D/01_real_data/AEON/benchmarks")
RESULTS_DIR = Path("/Users/franco/Documents/TabPFN3D/05_flattened_benchmark/results")
TABPFN_BASELINE_CSV = RESULTS_DIR / "baseline_benchmark_latest.csv"
USABLE_DATASETS_JSON = RESULTS_DIR / "usable_datasets.json"

# TabPFN finetuned result (from training output)
TABPFN_FINETUNED_AUC = 0.9282  # >> REAL EVAL: AUC=0.9282


def load_aeon_benchmarks():
    """Load all AEON benchmark files with AUC scores."""
    benchmark_files = list(AEON_BENCHMARKS_DIR.glob("*_auroc.csv"))
    
    all_benchmarks = {}
    
    for file in benchmark_files:
        model_name = file.stem.replace("_auroc", "")
        try:
            df = pd.read_csv(file)
            # First column is 'Resamples:' header, actual data starts from row 1
            # Let's check the structure
            if df.columns[0] == 'Resamples:':
                # Standard format: first col is dataset name (after header), rest are runs
                df = pd.read_csv(file, index_col=0)
                df.index.name = 'dataset'
            else:
                df = pd.read_csv(file, index_col=0)
                df.index.name = 'dataset'
            
            # Calculate mean AUC across all resamples (columns)
            mean_auc = df.mean(axis=1)
            all_benchmarks[model_name] = mean_auc
            
        except Exception as e:
            print(f"Error loading {file}: {e}")
            continue
    
    return all_benchmarks


def load_tabpfn_baseline():
    """Load TabPFN baseline results."""
    df = pd.read_csv(TABPFN_BASELINE_CSV)
    # Filter successful runs and get AUC
    success_df = df[df['status'] == 'success'].copy()
    # Create series with dataset name as index
    tabpfn_auc = success_df.set_index('name')['roc_auc']
    return tabpfn_auc


def load_flattenable_datasets():
    """Load list of flattenable datasets."""
    with open(USABLE_DATASETS_JSON) as f:
        data = json.load(f)
    
    # Get all dataset names
    flattenable_names = [d['name'] for d in data['datasets']]
    return flattenable_names


def main():
    print("=" * 80)
    print("AEON vs TabPFN Comparison Analysis (Flattenable Datasets)")
    print("=" * 80)
    
    # Load data
    print("\n1. Loading AEON benchmarks...")
    aeon_benchmarks = load_aeon_benchmarks()
    print(f"   Loaded {len(aeon_benchmarks)} AEON models")
    
    print("\n2. Loading TabPFN baseline...")
    tabpfn_baseline = load_tabpfn_baseline()
    print(f"   Loaded {len(tabpfn_baseline)} TabPFN baseline results")
    
    print("\n3. Loading flattenable datasets list...")
    flattenable_datasets = load_flattenable_datasets()
    print(f"   {len(flattenable_datasets)} flattenable datasets identified")
    
    # Create combined DataFrame with all models
    print("\n4. Creating combined comparison DataFrame...")
    
    # Start with AEON benchmarks
    combined_df = pd.DataFrame(aeon_benchmarks)
    
    # Add TabPFN baseline
    combined_df['TabPFN_Baseline'] = tabpfn_baseline
    
    # Show available datasets
    print(f"\n   Total datasets in AEON: {len(combined_df)}")
    
    # Filter to flattenable datasets only
    flattenable_in_aeon = [d for d in flattenable_datasets if d in combined_df.index]
    print(f"   Flattenable datasets found in AEON: {len(flattenable_in_aeon)}")
    
    # Also check which flattenable datasets have TabPFN results
    flattenable_with_tabpfn = [d for d in flattenable_in_aeon if d in tabpfn_baseline.index]
    print(f"   Flattenable datasets with TabPFN baseline: {len(flattenable_with_tabpfn)}")
    
    # Filter DataFrame to flattenable datasets with TabPFN results
    filtered_df = combined_df.loc[flattenable_with_tabpfn].copy()
    
    print(f"\n   Working with {len(filtered_df)} datasets for comparison")
    
    # Print which datasets we're using
    print("\n   Datasets included:")
    for i, name in enumerate(filtered_df.index):
        print(f"     {i+1}. {name}")
    
    # Calculate mean AUC per model across flattenable datasets
    print("\n" + "=" * 80)
    print("5. MEAN AUC ACROSS FLATTENABLE DATASETS (with TabPFN baseline)")
    print("=" * 80)
    
    model_means = filtered_df.mean().sort_values(ascending=False)
    
    print("\nRanking of models by mean AUC:")
    print("-" * 50)
    for i, (model, auc) in enumerate(model_means.items(), 1):
        marker = " <<<" if model == 'TabPFN_Baseline' else ""
        print(f"{i:3d}. {model:30s}: {auc:.4f}{marker}")
    
    # Find TabPFN rank
    tabpfn_rank = list(model_means.index).index('TabPFN_Baseline') + 1
    total_models = len(model_means)
    print(f"\n>>> TabPFN Baseline Rank: {tabpfn_rank}/{total_models}")
    print(f">>> TabPFN Baseline Mean AUC: {model_means['TabPFN_Baseline']:.4f}")
    
    # Compare with TabPFN Finetuned
    print("\n" + "=" * 80)
    print("6. COMPARISON WITH TABPFN FINETUNED")
    print("=" * 80)
    
    print(f"\n   TabPFN Finetuned Mean AUC: {TABPFN_FINETUNED_AUC:.4f}")
    
    # Where would finetuned TabPFN rank?
    models_below_finetuned = sum(1 for auc in model_means if auc < TABPFN_FINETUNED_AUC)
    finetuned_rank = total_models - models_below_finetuned
    print(f"   TabPFN Finetuned would rank: ~{finetuned_rank}/{total_models}")
    
    # How many models does it beat?
    print(f"\n   Models beaten by TabPFN Finetuned: {models_below_finetuned}/{total_models}")
    
    # Improvement over baseline
    baseline_auc = model_means['TabPFN_Baseline']
    improvement = (TABPFN_FINETUNED_AUC - baseline_auc) * 100
    print(f"\n   Improvement over TabPFN Baseline: +{improvement:.2f} percentage points")
    
    # Top 10 models including where finetuned would be
    print("\n" + "=" * 80)
    print("7. TOP MODELS (with TabPFN Finetuned position)")
    print("=" * 80)
    
    # Create ranking with finetuned
    model_means_with_finetuned = model_means.copy()
    model_means_with_finetuned['TabPFN_Finetuned'] = TABPFN_FINETUNED_AUC
    model_means_with_finetuned = model_means_with_finetuned.sort_values(ascending=False)
    
    print("\nTop 20 models (including TabPFN Finetuned):")
    print("-" * 55)
    for i, (model, auc) in enumerate(model_means_with_finetuned.head(20).items(), 1):
        if model == 'TabPFN_Finetuned':
            marker = " <<< FINETUNED"
        elif model == 'TabPFN_Baseline':
            marker = " <<< BASELINE"
        else:
            marker = ""
        print(f"{i:3d}. {model:30s}: {auc:.4f}{marker}")
    
    # Detailed comparison table
    print("\n" + "=" * 80)
    print("8. DETAILED COMPARISON: TOP 10 TS MODELS vs TABPFN")
    print("=" * 80)
    
    # Select top 10 time series models (excluding TabPFN)
    top_ts_models = [m for m in model_means.index if 'TabPFN' not in m][:10]
    
    comparison_data = []
    for model in top_ts_models:
        model_auc = model_means[model]
        diff_vs_baseline = model_auc - baseline_auc
        diff_vs_finetuned = model_auc - TABPFN_FINETUNED_AUC
        comparison_data.append({
            'Model': model,
            'Mean AUC': model_auc,
            'vs TabPFN Base': diff_vs_baseline,
            'vs TabPFN Fine': diff_vs_finetuned
        })
    
    comparison_table = pd.DataFrame(comparison_data)
    print("\n" + comparison_table.to_string(index=False))
    
    # Per-dataset comparison
    print("\n" + "=" * 80)
    print("9. PER-DATASET ANALYSIS")
    print("=" * 80)
    
    # Compare TabPFN baseline vs best AEON model per dataset
    best_aeon_per_dataset = filtered_df.drop(columns=['TabPFN_Baseline']).max(axis=1)
    tabpfn_vs_best = filtered_df['TabPFN_Baseline'] - best_aeon_per_dataset
    
    print("\nDatasets where TabPFN Baseline beats ALL AEON models:")
    ds_wins = tabpfn_vs_best[tabpfn_vs_best > 0]
    n_ds_wins = len(ds_wins)
    print(f"   {n_ds_wins} datasets")
    for ds in ds_wins.index:
        print(f"   - {ds}: TabPFN={filtered_df.loc[ds, 'TabPFN_Baseline']:.4f}, Best AEON={best_aeon_per_dataset[ds]:.4f}")
    
    print("\nDatasets where TabPFN Baseline is worst:")
    worst_5 = tabpfn_vs_best.nsmallest(5)
    for ds, diff in worst_5.items():
        print(f"   - {ds}: TabPFN={filtered_df.loc[ds, 'TabPFN_Baseline']:.4f}, Best AEON={best_aeon_per_dataset[ds]:.4f} (diff: {diff:.4f})")
    
    # Summary statistics
    print("\n" + "=" * 80)
    print("10. SUMMARY STATISTICS")
    print("=" * 80)
    
    print(f"\nNumber of flattenable datasets analyzed: {len(filtered_df)}")
    print(f"Number of AEON models compared: {len(aeon_benchmarks)}")
    
    print(f"\nTabPFN Baseline:")
    print(f"   Mean AUC: {baseline_auc:.4f}")
    print(f"   Rank: {tabpfn_rank}/{total_models}")
    
    finetuned_rank_final = list(model_means_with_finetuned.index).index('TabPFN_Finetuned') + 1
    print(f"\nTabPFN Finetuned:")
    print(f"   Mean AUC: {TABPFN_FINETUNED_AUC:.4f}")
    print(f"   Rank: {finetuned_rank_final}/{total_models + 1}")
    print(f"   Improvement over baseline: +{improvement:.2f}pp")
    
    # Best AEON model
    best_model = model_means.index[0]
    print(f"\nBest AEON model: {best_model}")
    print(f"   Mean AUC: {model_means[best_model]:.4f}")
    
    # Percentile of TabPFN
    percentile_baseline = (1 - tabpfn_rank / total_models) * 100
    percentile_finetuned = (1 - finetuned_rank_final / (total_models + 1)) * 100
    print(f"\nPercentile ranking:")
    print(f"   TabPFN Baseline: Top {100-percentile_baseline:.1f}%")
    print(f"   TabPFN Finetuned: Top {100-percentile_finetuned:.1f}%")
    
    # Additional Analysis: Win/Tie/Loss analysis
    print("\n" + "=" * 80)
    print("11. WIN/TIE/LOSS ANALYSIS vs TOP MODELS")
    print("=" * 80)
    
    top_models = ['HC2', 'InceptionTime', 'ROCKET', 'ResNet', 'BOSS']
    top_models = [m for m in top_models if m in filtered_df.columns]
    
    for top_model in top_models:
        wins = sum(filtered_df['TabPFN_Baseline'] > filtered_df[top_model])
        ties = sum(abs(filtered_df['TabPFN_Baseline'] - filtered_df[top_model]) < 0.001)
        losses = sum(filtered_df['TabPFN_Baseline'] < filtered_df[top_model])
        print(f"\n   TabPFN vs {top_model}:")
        print(f"      Wins: {wins}, Ties: {ties}, Losses: {losses}")
    
    # Analysis by dataset characteristics
    print("\n" + "=" * 80)
    print("12. ANALYSIS BY DATASET CHARACTERISTICS")
    print("=" * 80)
    
    # Load dataset metadata
    flattenable_df = pd.read_csv(RESULTS_DIR / "flattenable_summary.csv")
    
    # Create feature-based groups
    for ds in filtered_df.index:
        if ds in flattenable_df['name'].values:
            ds_info = flattenable_df[flattenable_df['name'] == ds].iloc[0]
            filtered_df.loc[ds, 'flattened_features'] = ds_info['flattened_features']
            filtered_df.loc[ds, 'n_classes'] = ds_info['n_classes']
    
    # Group by feature count
    low_features = filtered_df[filtered_df['flattened_features'] <= 100]
    mid_features = filtered_df[(filtered_df['flattened_features'] > 100) & (filtered_df['flattened_features'] <= 300)]
    high_features = filtered_df[filtered_df['flattened_features'] > 300]
    
    print("\nTabPFN Baseline Mean AUC by Feature Count:")
    if len(low_features) > 0:
        print(f"   Low features (â‰¤100): {low_features['TabPFN_Baseline'].mean():.4f} ({len(low_features)} datasets)")
    if len(mid_features) > 0:
        print(f"   Mid features (100-300): {mid_features['TabPFN_Baseline'].mean():.4f} ({len(mid_features)} datasets)")
    if len(high_features) > 0:
        print(f"   High features (>300): {high_features['TabPFN_Baseline'].mean():.4f} ({len(high_features)} datasets)")
    
    # Group by number of classes
    binary = filtered_df[filtered_df['n_classes'] == 2]
    multiclass = filtered_df[filtered_df['n_classes'] > 2]
    
    print("\nTabPFN Baseline Mean AUC by Classification Type:")
    if len(binary) > 0:
        print(f"   Binary (2 classes): {binary['TabPFN_Baseline'].mean():.4f} ({len(binary)} datasets)")
    if len(multiclass) > 0:
        print(f"   Multiclass (>2 classes): {multiclass['TabPFN_Baseline'].mean():.4f} ({len(multiclass)} datasets)")
    
    # Gap analysis
    print("\n" + "=" * 80)
    print("13. GAP ANALYSIS: TABPFN vs STATE-OF-THE-ART")
    print("=" * 80)
    
    sota_models = ['HC2', 'RIST', 'HC1', 'DrCIF', 'InceptionTime']
    sota_models = [m for m in sota_models if m in model_means.index]
    
    if sota_models:
        sota_mean = model_means[sota_models].mean()
        gap_baseline = sota_mean - baseline_auc
        gap_finetuned = sota_mean - TABPFN_FINETUNED_AUC
        
        print(f"\nSOTA average (top 5 models): {sota_mean:.4f}")
        print(f"TabPFN Baseline gap to SOTA: {gap_baseline:.4f} ({gap_baseline*100:.2f}pp)")
        print(f"TabPFN Finetuned gap to SOTA: {gap_finetuned:.4f} ({gap_finetuned*100:.2f}pp)")
        print(f"Gap reduction from finetuning: {(gap_baseline - gap_finetuned)*100:.2f}pp ({(1-gap_finetuned/gap_baseline)*100:.1f}% of gap closed)")
    
    # Key insights summary
    print("\n" + "=" * 80)
    print("14. KEY INSIGHTS SUMMARY")
    print("=" * 80)
    
    print(f"""
    1. TabPFN Baseline Performance:
       - Ranks {tabpfn_rank}/{total_models} among TS classification models
       - Mean AUC: {baseline_auc:.4f}
       - Competitive with traditional TS models, but below deep learning SOTA
    
    2. TabPFN Finetuned Performance:
       - Would rank {finetuned_rank_final}/{total_models + 1}
       - Mean AUC: {TABPFN_FINETUNED_AUC:.4f}
       - Improvement: +{improvement:.2f}pp over baseline
       - Beats {models_below_finetuned} additional models after finetuning
    
    3. Dataset-specific Strengths:
       - TabPFN excels on datasets with clear tabular-like patterns
       - Beats ALL AEON models on {n_ds_wins} datasets
       - Struggles with shapelet-based patterns (ShapeletSim: 0.47 vs 1.00)
    
    4. Finetuning Impact:
       - Closes ~{(1-gap_finetuned/gap_baseline)*100:.0f}% of gap to SOTA (when available)
       - Most gain in mid-complexity datasets
       - Still competitive with specialized TS methods
    
    5. Practical Implications:
       - TabPFN is a strong zero-shot baseline for flattenable TS
       - Finetuning provides meaningful improvements
       - For production, consider ensemble with top TS models
    """)
    
    # Save results
    output_path = RESULTS_DIR / "aeon_comparison_results.csv"
    model_means_with_finetuned.to_csv(output_path)
    print(f"\nResults saved to: {output_path}")
    
    # Save detailed per-dataset comparison
    detailed_output = RESULTS_DIR / "aeon_detailed_comparison.csv"
    filtered_df.to_csv(detailed_output)
    print(f"Detailed results saved to: {detailed_output}")
    
    return filtered_df, model_means_with_finetuned


if __name__ == "__main__":
    filtered_df, rankings = main()
